<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in Diffusion Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sooyeon-go.github.io/">Sooyeon Go</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chkmook.github.io/">Kyungmook Choi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://minjung-s.github.io/">Minjung Shin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://vilab.yonsei.ac.kr/member/professor">Youngjung Uh</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sooyeon-go/eye_for_an_eye"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png"
                 class="teaser"
                 alt="teaser"/>
      <h2 class="subtitle has-text-centered">
        Ours transfers semantically corresponding appearances from reference images to target images.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As pretrained text-to-image diffusion models have become a useful tool for image synthesis, 
            people want to specify the results in various ways. 
            In this paper, we introduce a method to produce results with the same structure of a target image 
            but painted with colors from a reference image, especially following the semantic correspondence between the result and the reference. 
            E.g., the result wing takes color from the reference wing, not the reference head. 
            Existing methods rely on the query-key similarity within self-attention layer, 
            usually producing defective results. To this end, we propose to find semantic correspondences and explicitly rearrange the features according to the semantic correspondences. 
            Extensive experiments show the superiority of our method in various aspects: 
            preserving the structure of the target and reflecting the color from the reference according to the semantic correspondences, even when the two images are not aligned.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall Framework</h2>
        <div class="overal_framework">
          <img src="static/images/framework.png"
                 class="framework"
                 alt="framework"/>
          <p>We transfer the semantically corresponding appearance of objects from a reference image to a target image.
          Given \( I^\text{ref} \), \( I^\text{target} \), and their masks \( M^\text{ref} \) and \( M^\text{target} \),
          we find semantic correspondences between their self-attention features \( F^\text{ref}_t \) and \( F^\text{out}_t \).
          Then, we inject the rearranged features based on these correspondences.</p>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">\( KV \) vs \( V \) vs \( Feature \) Injection</h2>
        <div class="overal_framework">
          <img src="static/images/kv_v_f.png"
                 class="teaser"
                 alt="teaser" style="width: 120%;"/>
          <p>We provide the attention maps for the target image's query pixel (red dot) at different timesteps during appearance transfer. 
            The \( KV \) injection(first row) and \( V \) injection(second row) perform semantic matching in the same manner as our method 
            but apply the rearrangement and injection processes to \( KV \) and \( V \) instead of the feature map, respectively.</p>
          <p>
            Query of \( KV \) and \( V \) injection recognizes belly parts with different colors as different parts, leading to bring different appearance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <div class="columns is-centered">
      <div class="column is-12">
        <figure class="image" style="display: flex; justify-content: center;">
          <img src="static/images/results.png" alt="result" style="width: 70%; max-width: 1200px;">
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Application</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Cross-style appearance transfer</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <img src="./static/images/cross_image.png"
                 class="cross style image"
                 alt="cross image transfer."/>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Multi-objects appearance transfer</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/multi_object.png"
                 class="multi object image"
                 alt="multi object transfer."/>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/sooyeon-go/eye_for_an_eye" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We borrow the website template from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
